<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
      <title>
        Language Models
      </title>
      <meta property="og:title" content="Language Models">
    <meta charset="utf-8">
    <meta property="og:type" content="article">

      <meta property="og:description" content="Short description of your project">
      <meta property="description" content="Short description of your project">

    <link rel="stylesheet" href="styles.css">
  </head>
  <body>
    <div id="idyll-mount"><div data-reactroot=""><div class="idyll-root"><div style="max-width:600px;margin:0 0 0 50px" class=" idyll-text-container"><div class="article-header"><h1 class="hed">Language Modelling</h1><h2 class="dek">Part I: Character Level Language Models</h2><div class="byline">By: <a href="https://twitter.com/viiitdmj">Vineet Kumar Singh</a></div></div><p>How can we teach computers to understand text. Recently, there has been many breakthroughs in the field on <strong>Natural Language Processing (nlp)</strong>.
I am especially interested to see the future of <strong>transfer learning for nlp</strong>.</p><p>To quote <a href="https://twitter.com/ch402">amazing Chris Olah</a>: “Humans don’t start their thinking from scratch
every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start
thinking from scratch again. Your thoughts have persistence.” How can we build this basic level of state tracking to predict the next character based
on character typed so far, is the main topic of this essay.</p><div class="aside-container"><div class="aside">
  ML5 LSTM generator place holder</div></div></div></div></div></div>
    <script src="index.js"></script>
  </body>
</html>
